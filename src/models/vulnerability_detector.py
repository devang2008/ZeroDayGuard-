"""
Simple Neural Network Model for Vulnerability Detection
Uses combined features from code and graphs
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class VulnerabilityDetector(nn.Module):
    """
    Simple feedforward neural network for vulnerability detection
    Combines code features and graph features
    """
    
    def __init__(self, 
                 code_feature_dim: int = 20,
                 graph_feature_dim: int = 8,
                 hidden_dim: int = 128,
                 num_classes: int = 2,
                 dropout: float = 0.3):
        """
        Initialize model
        
        Args:
            code_feature_dim: Dimension of code features
            graph_feature_dim: Dimension of each graph feature vector
            hidden_dim: Hidden layer dimension
            num_classes: Number of output classes (2 for binary)
            dropout: Dropout rate
        """
        super(VulnerabilityDetector, self).__init__()
        
        # Input dimension = code features + 3 graph features (AST, CFG, PDG)
        input_dim = code_feature_dim + (3 * graph_feature_dim)
        
        # Feature processing layers
        self.fc1 = nn.Linear(input_dim, hidden_dim * 2)
        self.bn1 = nn.BatchNorm1d(hidden_dim * 2)
        self.dropout1 = nn.Dropout(dropout)
        
        self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim)
        self.bn2 = nn.BatchNorm1d(hidden_dim)
        self.dropout2 = nn.Dropout(dropout)
        
        self.fc3 = nn.Linear(hidden_dim, hidden_dim // 2)
        self.bn3 = nn.BatchNorm1d(hidden_dim // 2)
        self.dropout3 = nn.Dropout(dropout)
        
        # Output layer
        self.fc_out = nn.Linear(hidden_dim // 2, num_classes)
    
    def forward(self, code_features, ast_features, cfg_features, pdg_features):
        """
        Forward pass
        
        Args:
            code_features: Tensor of shape (batch_size, code_feature_dim)
            ast_features: Tensor of shape (batch_size, graph_feature_dim)
            cfg_features: Tensor of shape (batch_size, graph_feature_dim)
            pdg_features: Tensor of shape (batch_size, graph_feature_dim)
            
        Returns:
            Logits of shape (batch_size, num_classes)
        """
        # Concatenate all features
        x = torch.cat([code_features, ast_features, cfg_features, pdg_features], dim=1)
        
        # Layer 1
        x = self.fc1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.dropout1(x)
        
        # Layer 2
        x = self.fc2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.dropout2(x)
        
        # Layer 3
        x = self.fc3(x)
        x = self.bn3(x)
        x = F.relu(x)
        x = self.dropout3(x)
        
        # Output
        logits = self.fc_out(x)
        
        return logits


class ImprovedVulnerabilityDetector(nn.Module):
    """
    Improved model with attention mechanism
    """
    
    def __init__(self,
                 code_feature_dim: int = 20,
                 graph_feature_dim: int = 8,
                 hidden_dim: int = 128,
                 num_classes: int = 2,
                 dropout: float = 0.3):
        super(ImprovedVulnerabilityDetector, self).__init__()
        
        # Separate processing for each feature type
        self.code_encoder = nn.Sequential(
            nn.Linear(code_feature_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        self.graph_encoder = nn.Sequential(
            nn.Linear(graph_feature_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # Attention weights for graph features
        self.attention = nn.Linear(3 * (hidden_dim // 2), 3)
        
        # Combined processing
        combined_dim = hidden_dim + (hidden_dim // 2)
        
        self.classifier = nn.Sequential(
            nn.Linear(combined_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_classes)
        )
    
    def forward(self, code_features, ast_features, cfg_features, pdg_features):
        """Forward pass with attention"""
        # Encode code features
        code_emb = self.code_encoder(code_features)
        
        # Encode graph features separately
        ast_emb = self.graph_encoder(ast_features)
        cfg_emb = self.graph_encoder(cfg_features)
        pdg_emb = self.graph_encoder(pdg_features)
        
        # Concatenate graph embeddings for attention
        graph_embs = torch.cat([ast_emb, cfg_emb, pdg_emb], dim=1)
        
        # Calculate attention weights
        attn_weights = F.softmax(self.attention(graph_embs), dim=1)
        
        # Apply attention
        weighted_ast = attn_weights[:, 0:1] * ast_emb
        weighted_cfg = attn_weights[:, 1:2] * cfg_emb
        weighted_pdg = attn_weights[:, 2:3] * pdg_emb
        
        # Combine weighted graph features
        graph_emb = weighted_ast + weighted_cfg + weighted_pdg
        
        # Combine code and graph embeddings
        combined = torch.cat([code_emb, graph_emb], dim=1)
        
        # Classify
        logits = self.classifier(combined)
        
        return logits


def create_model(model_type: str = 'simple', **kwargs):
    """
    Factory function to create models
    
    Args:
        model_type: 'simple' or 'improved'
        **kwargs: Model parameters
        
    Returns:
        PyTorch model
    """
    if model_type == 'simple':
        return VulnerabilityDetector(**kwargs)
    elif model_type == 'improved':
        return ImprovedVulnerabilityDetector(**kwargs)
    else:
        raise ValueError(f"Unknown model type: {model_type}")
