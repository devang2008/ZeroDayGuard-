import torch

print("="*70)
print("ğŸ® GPU Detection")
print("="*70)

print(f"\nğŸ“¦ PyTorch version: {torch.__version__}")
print(f"ğŸ” CUDA available: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"âœ… CUDA version: {torch.version.cuda}")
    print(f"âœ… GPU count: {torch.cuda.device_count()}")
    print(f"âœ… GPU name: {torch.cuda.get_device_name(0)}")
    
    total_memory = torch.cuda.get_device_properties(0).total_memory
    print(f"âœ… GPU memory: {total_memory / 1024**3:.1f} GB")
    
    # Test GPU
    print("\nğŸ§ª Testing GPU...")
    x = torch.randn(1000, 1000).cuda()
    y = torch.randn(1000, 1000).cuda()
    z = torch.matmul(x, y)
    print("âœ… GPU computation successful!")
    
    # Estimate speedup
    print("\nâš¡ GPU Training Benefits:")
    print("  â€¢ CPU training time: 12-15 hours")
    print("  â€¢ GPU training time: 3-4 hours (3-4x faster!)")
    print("  â€¢ Larger batch sizes possible")
    print("  â€¢ Better GPU utilization")
else:
    print("\nâŒ CUDA not available!")
    print("   Make sure:")
    print("   1. PyTorch with CUDA is installed")
    print("   2. NVIDIA drivers are up to date")
    print("   3. GPU is properly connected")
